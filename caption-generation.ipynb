{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n\nfrom tensorflow import keras \nfrom tensorflow.compat.v1.keras.backend import set_session\nimport sys, time, os, warnings \nimport numpy as np\nimport pandas as pd \nfrom collections import Counter \nwarnings.filterwarnings(\"ignore\")\nprint(\"python {}\".format(sys.version))\nprint(\"keras version {}\".format(keras.__version__)); del keras\nprint(\"tensorflow version {}\".format(tf.__version__))","metadata":{"id":"taXOG_6s0Ad2","outputId":"0fdf7967-c830-4969-9208-65e0d16c89c1","execution":{"iopub.status.busy":"2021-08-02T00:03:53.652277Z","iopub.execute_input":"2021-08-02T00:03:53.65266Z","iopub.status.idle":"2021-08-02T00:03:58.099852Z","shell.execute_reply.started":"2021-08-02T00:03:53.652561Z","shell.execute_reply":"2021-08-02T00:03:58.098388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nsess = tf.Session()\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.95 #Using 95% of the available memory of the GPU\nconfig.gpu_options.visible_device_list = \"0\"\nset_session(tf.Session(config=config))\n\ndef set_seed(sd=144):\n    from numpy.random import seed\n    from tensorflow import set_random_seed\n    import random as rn\n    ## numpy random seed\n    seed(sd)\n    ## core python's random number \n    rn.seed(sd)\n    ## tensor flow's random number\n    set_random_seed(sd)","metadata":{"id":"xCf_pP8Z2_78","execution":{"iopub.status.busy":"2021-08-02T00:10:36.018274Z","iopub.execute_input":"2021-08-02T00:10:36.018614Z","iopub.status.idle":"2021-08-02T00:10:36.038476Z","shell.execute_reply.started":"2021-08-02T00:10:36.018562Z","shell.execute_reply":"2021-08-02T00:10:36.037672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from google.colab import drive                                                  \ndrive.mount('/content/drive/', force_remount = True) ","metadata":{"id":"1Ur9uDXN3yB5","outputId":"5d004c0f-e02b-443d-ec1c-3b6033f52f9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finding the captions for each image.\nfile = open(dir_Flickr_text,'r', encoding='utf8')\ntext = file.read()\nfile.close()\n\n\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\") # Splitting the caption dataset at the required position\n    datatxt.append(w + [col[1].lower()])\n\ndf_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n\n\nuni_filenames = np.unique(df_txt.filename.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df_txt.filename.values).values())\nprint(df_txt[:5])","metadata":{"id":"pD7HAb1W4rfJ","outputId":"b1633d2a-600d-44e0-eb93-0de74a7798f8","execution":{"iopub.status.busy":"2021-08-02T00:11:30.765495Z","iopub.execute_input":"2021-08-02T00:11:30.765888Z","iopub.status.idle":"2021-08-02T00:11:33.055904Z","shell.execute_reply.started":"2021-08-02T00:11:30.765855Z","shell.execute_reply":"2021-08-02T00:11:33.054996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## The location of the Flickr8K_ images\ndir_Flickr_jpg = '../input/coco-2017-dataset/coco2017/train2017'\n## The location of the caption file\ndir_Flickr_text = '../input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\n\njpgs = os.listdir(dir_Flickr_jpg)\nprint(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))","metadata":{"id":"Z-uU3xDL3pwG","outputId":"fe53de64-3e42-4f00-bb39-6701b63f4d00","execution":{"iopub.status.busy":"2021-08-02T00:11:21.573361Z","iopub.execute_input":"2021-08-02T00:11:21.573703Z","iopub.status.idle":"2021-08-02T00:11:23.479331Z","shell.execute_reply.started":"2021-08-02T00:11:21.573671Z","shell.execute_reply":"2021-08-02T00:11:23.478524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom IPython.display import display\nfrom PIL import Image\n\nnpic = 5 # Displaying 5 images from the dataset\nnpix = 224\ntarget_size = (npix,npix,3)\n\ncount = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[-5:]:\n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    \n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n    \n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontsize=20)\n    count += 1\nplt.show()","metadata":{"id":"8YKSCc8S4ul4","outputId":"b5acd732-d9a7-40ba-fbf1-4ce40d1cd669","execution":{"iopub.status.busy":"2021-07-19T00:14:24.167115Z","iopub.execute_input":"2021-07-19T00:14:24.167475Z","iopub.status.idle":"2021-07-19T00:14:25.028695Z","shell.execute_reply.started":"2021-07-19T00:14:24.167446Z","shell.execute_reply":"2021-07-19T00:14:25.027679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to calculate the top 3 words in all the captions available for the images\ndef df_word(df_txt):\n    vocabulary = []\n    for txt in df_txt.caption.values:\n        vocabulary.extend(txt.split())\n    print('Vocabulary Size: %d' % len(set(vocabulary)))\n    ct = Counter(vocabulary)\n    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n    dfword = dfword.sort_values(\"count\",ascending=False)\n    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n    return(dfword)\ndfword = df_word(df_txt)\ndfword.head(3)","metadata":{"id":"b1JTPyuL4nnT","outputId":"1c7247ef-0e8c-495c-f7a4-6cbb34b0cf6d","execution":{"iopub.status.busy":"2021-07-19T00:14:30.569123Z","iopub.execute_input":"2021-07-19T00:14:30.569475Z","iopub.status.idle":"2021-07-19T00:14:30.717938Z","shell.execute_reply.started":"2021-07-19T00:14:30.569444Z","shell.execute_reply":"2021-07-19T00:14:30.71705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\ntext_original = \"I ate 1000 apples and a banana. I have python v2.7. It's 2:30 pm. Could you buy me iphone7?\"\n\nprint(text_original)\nprint(\"\\nRemove punctuations..\")\ndef remove_punctuation(text_original):\n    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n    return(text_no_punctuation)\ntext_no_punctuation = remove_punctuation(text_original)\nprint(text_no_punctuation)\n\n\nprint(\"\\nRemove a single character word..\")\ndef remove_single_character(text):\n    text_len_more_than1 = \"\"\n    for word in text.split():\n        if len(word) > 1:\n            text_len_more_than1 += \" \" + word\n    return(text_len_more_than1)\ntext_len_more_than1 = remove_single_character(text_no_punctuation)\nprint(text_len_more_than1)\n\nprint(\"\\nRemove words with numeric values..\")\ndef remove_numeric(text,printTF=False):\n    text_no_numeric = \"\"\n    for word in text.split():\n        isalpha = word.isalpha()\n        if printTF:\n            print(\"    {:10} : {:}\".format(word,isalpha))\n        if isalpha:\n            text_no_numeric += \" \" + word\n    return(text_no_numeric)\ntext_no_numeric = remove_numeric(text_len_more_than1,printTF=True)\nprint(text_no_numeric)","metadata":{"id":"PXk7lz_B5KHR","outputId":"bddb5456-62aa-4f67-b5b4-f0e9bf612cfa","execution":{"iopub.status.busy":"2021-07-19T00:14:35.489401Z","iopub.execute_input":"2021-07-19T00:14:35.489772Z","iopub.status.idle":"2021-07-19T00:14:35.503254Z","shell.execute_reply.started":"2021-07-19T00:14:35.48974Z","shell.execute_reply":"2021-07-19T00:14:35.502145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_clean(text_original):\n    text = remove_punctuation(text_original)\n    text = remove_single_character(text)\n    text = remove_numeric(text)\n    return(text)\n\n\nfor i, caption in enumerate(df_txt.caption.values):\n    newcaption = text_clean(caption)\n    df_txt[\"caption\"].iloc[i] = newcaption","metadata":{"id":"r-uTwK9W5M2S","execution":{"iopub.status.busy":"2021-07-19T00:14:39.443032Z","iopub.execute_input":"2021-07-19T00:14:39.443412Z","iopub.status.idle":"2021-07-19T00:14:53.700584Z","shell.execute_reply.started":"2021-07-19T00:14:39.443375Z","shell.execute_reply":"2021-07-19T00:14:53.699716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topn = 50\n\ndef plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(dfsub.index,dfsub[\"count\"])\n    plt.yticks(fontsize=20)\n    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\ndfword = df_word(df_txt)\nplthist(dfword.iloc[:topn,:],\n        title=\"The top 50 most frequently appearing words\")\nplthist(dfword.iloc[-topn:,:],\n        title=\"The least 50 most frequently appearing words\")","metadata":{"id":"xg5SuBCP5QJi","outputId":"81eeca4d-1eec-4395-a125-ba6d9f43845c","execution":{"iopub.status.busy":"2021-07-19T00:14:53.703801Z","iopub.execute_input":"2021-07-19T00:14:53.704071Z","iopub.status.idle":"2021-07-19T00:14:54.755568Z","shell.execute_reply.started":"2021-07-19T00:14:53.704046Z","shell.execute_reply":"2021-07-19T00:14:54.754568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom copy import copy\ndef add_start_end_seq_token(captions):\n    caps = []\n    for txt in captions:\n        txt = 'startseq ' + txt + ' endseq'\n        caps.append(txt)\n    return(caps)\ndf_txt0 = copy(df_txt)\ndf_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\ndf_txt0.head(5)\ndel df_txt","metadata":{"id":"3Y8oCBTl5XjZ","execution":{"iopub.status.busy":"2021-07-19T00:14:57.277039Z","iopub.execute_input":"2021-07-19T00:14:57.277401Z","iopub.status.idle":"2021-07-19T00:14:57.308822Z","shell.execute_reply.started":"2021-07-19T00:14:57.277361Z","shell.execute_reply":"2021-07-19T00:14:57.308028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_txt0[:5]","metadata":{"id":"fAMZ-zGK5alZ","outputId":"d1fe4c67-b0cb-462b-fe95-c9061f0d1f4a","execution":{"iopub.status.busy":"2021-07-19T00:15:01.17346Z","iopub.execute_input":"2021-07-19T00:15:01.17378Z","iopub.status.idle":"2021-07-19T00:15:01.183929Z","shell.execute_reply.started":"2021-07-19T00:15:01.173751Z","shell.execute_reply":"2021-07-19T00:15:01.182949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\n\nmodelvgg = VGG16(include_top=True,weights=None)\n## load the locally saved weights \nmodelvgg.load_weights('../input/vgg16model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\nmodelvgg.summary()","metadata":{"id":"IDa4spJm5dOi","outputId":"03fd6454-69a1-4dc3-ab4c-cc0d3605cfec","execution":{"iopub.status.busy":"2021-07-19T00:15:23.909847Z","iopub.execute_input":"2021-07-19T00:15:23.910202Z","iopub.status.idle":"2021-07-19T00:15:30.254382Z","shell.execute_reply.started":"2021-07-19T00:15:23.91015Z","shell.execute_reply":"2021-07-19T00:15:30.253586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import models\nmodelvgg.layers.pop()\nmodelvgg = models.Model(inputs=modelvgg.inputs, outputs=modelvgg.layers[-1].output)\n## show the deep learning model\nmodelvgg.summary()","metadata":{"id":"ReskQaoO0P-C","outputId":"2ec7a385-774b-400d-b084-de4f93bd4f52","execution":{"iopub.status.busy":"2021-07-19T00:15:33.529709Z","iopub.execute_input":"2021-07-19T00:15:33.530042Z","iopub.status.idle":"2021-07-19T00:15:33.552773Z","shell.execute_reply.started":"2021-07-19T00:15:33.530011Z","shell.execute_reply":"2021-07-19T00:15:33.552015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom collections import OrderedDict\n\nimages = OrderedDict()\nnpix = 224 #image size is fixed at 224 because VGG16 model has been pre-trained to take that size.\ntarget_size = (npix,npix,3)\ndata = np.zeros((len(jpgs),npix,npix,3))\nfor i,name in enumerate(jpgs):\n    # load an image from file\n    filename = dir_Flickr_jpg + '/' + name\n    image = load_img(filename, target_size=target_size)\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    nimage = preprocess_input(image)\n    \n    y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n    images[name] = y_pred.flatten()","metadata":{"id":"LzSuVp_M0390","execution":{"iopub.status.busy":"2021-07-19T00:15:37.232571Z","iopub.execute_input":"2021-07-19T00:15:37.232894Z","iopub.status.idle":"2021-07-19T00:22:54.405751Z","shell.execute_reply.started":"2021-07-19T00:15:37.232864Z","shell.execute_reply":"2021-07-19T00:22:54.404647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nencoder = np.array(list(images.values()))\n#print(encoder)\npca = PCA(n_components=2)\n#print(pca)\ny_pca = pca.fit_transform(encoder)","metadata":{"id":"89Zg7j9MGfNA","execution":{"iopub.status.busy":"2021-07-19T00:29:18.120686Z","iopub.execute_input":"2021-07-19T00:29:18.121095Z","iopub.status.idle":"2021-07-19T00:29:19.457282Z","shell.execute_reply.started":"2021-07-19T00:29:18.12104Z","shell.execute_reply":"2021-07-19T00:29:19.456192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## some selected pictures that are creating clusters\n#these are just to display the related images from the dataset\npicked_pic = OrderedDict()\npicked_pic[\"red\"]     = [2720,4250,4983,5862,4079]\npicked_pic[\"green\"]   = [2070,3784,7545,4644, 4997]\npicked_pic[\"magenta\"] = [6320,3432,1348,7472, 1518]\npicked_pic[\"blue\"]    = [3901,2168,3465,5285,5328]\npicked_pic[\"yellow\"]  = [144,1172,4423,4780,4448]\npicked_pic[\"purple\"]  = [5087]\n\nfig, ax = plt.subplots(figsize=(15,15))\nax.scatter(y_pca[:,0],y_pca[:,1],c=\"white\")\n\nfor irow in range(y_pca.shape[0]):\n    ax.annotate(irow,y_pca[irow,:],color=\"black\",alpha=0.5) #annotate() is used to place text at the location of the point\nfor color, irows in picked_pic.items():\n    for irow in irows:\n        ax.annotate(irow,y_pca[irow,:],color=color)\nax.set_xlabel(\"pca embedding 1\",fontsize=30)\nax.set_ylabel(\"pca embedding 2\",fontsize=30)\nplt.show()\n\n\n## plot of images \nfig = plt.figure(figsize=(16,20))\ncount = 1\nfor color, irows in picked_pic.items():\n    for ivec in irows:\n        name = jpgs[ivec]\n        filename = dir_Flickr_jpg + '/' + name\n        image = load_img(filename, target_size=target_size)\n    \n        ax = fig.add_subplot(len(picked_pic),5,count,\n                         xticks=[],yticks=[])\n        count += 1\n        plt.imshow(image)\n        plt.title(\"{} ({})\".format(ivec,color))\nplt.show()","metadata":{"id":"O-SmbmbLGp0I","outputId":"9783168d-e60a-430f-c9d7-c20f26867b62","execution":{"iopub.status.busy":"2021-07-19T00:29:22.913342Z","iopub.execute_input":"2021-07-19T00:29:22.91367Z","iopub.status.idle":"2021-07-19T00:30:07.514536Z","shell.execute_reply.started":"2021-07-19T00:29:22.913642Z","shell.execute_reply":"2021-07-19T00:30:07.513693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dimages, keepindex = [],[]\n# Creating a datframe where only first caption is taken for processing\ndf_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\nfor i, fnm in enumerate(df_txt0.filename):\n    if fnm in images.keys():\n        dimages.append(images[fnm])\n        keepindex.append(i)\n\n#fnames are the names of the image files        \nfnames = df_txt0[\"filename\"].iloc[keepindex].values\n#dcaptions are the captions of the images \ndcaptions = df_txt0[\"caption\"].iloc[keepindex].values\n#dimages are the actual features of the images\ndimages = np.array(dimages)","metadata":{"id":"myqMI7T7Gu-R","execution":{"iopub.status.busy":"2021-07-19T00:35:35.10736Z","iopub.execute_input":"2021-07-19T00:35:35.107779Z","iopub.status.idle":"2021-07-19T00:35:35.164955Z","shell.execute_reply.started":"2021-07-19T00:35:35.107744Z","shell.execute_reply":"2021-07-19T00:35:35.16405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dimages","metadata":{"id":"ud5gw2kfAoH5","outputId":"9036b3ca-1afd-4e97-8c55-459c4b3a01e7","execution":{"iopub.status.busy":"2021-07-19T00:35:41.120953Z","iopub.execute_input":"2021-07-19T00:35:41.121285Z","iopub.status.idle":"2021-07-19T00:35:41.128073Z","shell.execute_reply.started":"2021-07-19T00:35:41.121254Z","shell.execute_reply":"2021-07-19T00:35:41.127044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_txt0[:5]","metadata":{"id":"5chwwhR5G5yI","outputId":"6134ac32-a638-4539-9bcf-c60f8edff0bd","execution":{"iopub.status.busy":"2021-07-19T00:35:44.024252Z","iopub.execute_input":"2021-07-19T00:35:44.024577Z","iopub.status.idle":"2021-07-19T00:35:44.037692Z","shell.execute_reply.started":"2021-07-19T00:35:44.024549Z","shell.execute_reply":"2021-07-19T00:35:44.036579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n## the maximum number of words in dictionary\nnb_words = 6000\ntokenizer = Tokenizer(nb_words=nb_words)\ntokenizer.fit_on_texts(dcaptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocabulary size : {}\".format(vocab_size))\ndtexts = tokenizer.texts_to_sequences(dcaptions)\nprint(dtexts[:5])","metadata":{"id":"hsd6k4BgG9GT","outputId":"67e90eab-9636-4a8b-80e1-a47c379f2f53","execution":{"iopub.status.busy":"2021-07-19T00:35:46.831568Z","iopub.execute_input":"2021-07-19T00:35:46.831879Z","iopub.status.idle":"2021-07-19T00:35:47.104485Z","shell.execute_reply.started":"2021-07-19T00:35:46.83185Z","shell.execute_reply":"2021-07-19T00:35:47.103163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop_test, prop_val = 0.2, 0.2 \n\nN = len(dtexts)\nNtest, Nval = int(N*prop_test), int(N*prop_val)\n\ndef split_test_val_train(dtexts,Ntest,Nval):\n    return(dtexts[:Ntest], \n           dtexts[Ntest:Ntest+Nval],  \n           dtexts[Ntest+Nval:])\n\ndt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\ndi_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\nfnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)","metadata":{"id":"j7PUFS5mHED_","outputId":"c0cfe4e4-1d81-4bd7-c232-72e8dfd5eb33","execution":{"iopub.status.busy":"2021-07-19T00:35:49.652474Z","iopub.execute_input":"2021-07-19T00:35:49.65279Z","iopub.status.idle":"2021-07-19T00:35:49.661044Z","shell.execute_reply.started":"2021-07-19T00:35:49.652761Z","shell.execute_reply":"2021-07-19T00:35:49.660007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = np.max([len(text) for text in dtexts])\nprint(maxlen)","metadata":{"id":"V8WiqFJrHG4H","outputId":"045b2508-25df-4c51-eff0-5db749c5d794","execution":{"iopub.status.busy":"2021-07-19T00:35:52.957996Z","iopub.execute_input":"2021-07-19T00:35:52.958344Z","iopub.status.idle":"2021-07-19T00:35:52.966662Z","shell.execute_reply.started":"2021-07-19T00:35:52.958314Z","shell.execute_reply":"2021-07-19T00:35:52.965617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\ndef preprocessing(dtexts,dimages):\n    N = len(dtexts)\n    print(\"# captions/images = {}\".format(N))\n\n    assert(N==len(dimages)) # using assert to make sure that length of images and captions are always similar\n    Xtext, Ximage, ytext = [],[],[]\n    for text,image in zip(dtexts,dimages):\n        # zip() is used to create a tuple of iteratable items\n        for i in range(1,len(text)):\n            in_text, out_text = text[:i], text[i]\n            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()# using pad sequence to make the length of all captions equal\n            out_text = to_categorical(out_text,num_classes = vocab_size) # using to_categorical to \n\n            \n            Xtext.append(in_text)\n            Ximage.append(image)\n            ytext.append(out_text)\n\n    Xtext  = np.array(Xtext)\n    Ximage = np.array(Ximage)\n    ytext  = np.array(ytext)\n    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n    return(Xtext,Ximage,ytext)\n\n\nXtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\nXtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n# pre-processing is not necessary for testing data\n#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)","metadata":{"id":"OK2vA4ktHHiP","outputId":"15a46b0b-e53e-49bc-d33e-af6e174915f9","execution":{"iopub.status.busy":"2021-07-19T00:35:55.399059Z","iopub.execute_input":"2021-07-19T00:35:55.399477Z","iopub.status.idle":"2021-07-19T00:35:59.156861Z","shell.execute_reply.started":"2021-07-19T00:35:55.399446Z","shell.execute_reply":"2021-07-19T00:35:59.155935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import layers\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nprint(vocab_size)\n## image feature\n\ndim_embedding = 64\n\ninput_image = layers.Input(shape=(Ximage_train.shape[1],))\nfimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n## sequence model\ninput_txt = layers.Input(shape=(maxlen,))\nftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\nftxt = layers.LSTM(256,name=\"CaptionFeature\",return_sequences=True)(ftxt)\n#,return_sequences=True\n#,activation='relu'\nse2 = Dropout(0.04)(ftxt)\nftxt = layers.LSTM(256,name=\"CaptionFeature2\")(se2)\n## combined model for decoder\ndecoder = layers.add([ftxt,fimage])\ndecoder = layers.Dense(256,activation='relu')(decoder)\noutput = layers.Dense(vocab_size,activation='softmax')(decoder)\nmodel = models.Model(inputs=[input_image, input_txt],outputs=output)\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nprint(model.summary())\n","metadata":{"id":"6G9hopWjHYqM","execution":{"iopub.status.busy":"2021-07-19T01:39:39.374848Z","iopub.execute_input":"2021-07-19T01:39:39.375188Z","iopub.status.idle":"2021-07-19T01:39:40.766149Z","shell.execute_reply.started":"2021-07-19T01:39:39.37514Z","shell.execute_reply":"2021-07-19T01:39:40.764594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nfrom keras.callbacks import TensorBoard\ntensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n#start = time.time()\nhist = model.fit([Ximage_train, Xtext_train], ytext_train, \n                  epochs=20, verbose=2, \n                  batch_size=32,\n                  validation_data=([Ximage_val, Xtext_val], ytext_val),callbacks=[tensorboard])\nmodel.save(\"model_caption.h5\") \n#end = time.time()\n#print(\"TIME TOOK {:3.2f}MIN\".format((end - start )/60))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T01:41:04.90539Z","iopub.execute_input":"2021-07-19T01:41:04.905717Z","iopub.status.idle":"2021-07-19T03:01:50.529571Z","shell.execute_reply.started":"2021-07-19T01:41:04.905687Z","shell.execute_reply":"2021-07-19T03:01:50.528649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for label in [\"loss\",\"val_loss\"]:\n    plt.plot(hist.history[label],label=label)\nplt.legend()\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T01:00:50.088528Z","iopub.execute_input":"2021-07-19T01:00:50.088885Z","iopub.status.idle":"2021-07-19T01:00:50.250109Z","shell.execute_reply.started":"2021-07-19T01:00:50.088854Z","shell.execute_reply":"2021-07-19T01:00:50.249169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\ndef predict_caption(image):\n    '''\n    image.shape = (1,4462)\n    '''\n\n    in_text = 'startseq'\n\n    for iword in range(maxlen):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence],maxlen)\n        yhat = model.predict([image,sequence],verbose=0)\n        yhat = np.argmax(yhat)\n        newword = index_word[yhat]\n        in_text += \" \" + newword\n        if newword == \"endseq\":\n            break\n    return(in_text)\n\n\n\nnpic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T01:01:22.600646Z","iopub.execute_input":"2021-07-19T01:01:22.600975Z","iopub.status.idle":"2021-07-19T01:01:22.609189Z","shell.execute_reply.started":"2021-07-19T01:01:22.600945Z","shell.execute_reply":"2021-07-19T01:01:22.60809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm, image_feature in zip(fnm_test[7:12],di_test[7:12]):\n    ## images \n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0,0.5,caption,fontsize=20)\n    count += 1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:02:47.440561Z","iopub.execute_input":"2021-07-19T03:02:47.440872Z","iopub.status.idle":"2021-07-19T03:02:51.900146Z","shell.execute_reply.started":"2021-07-19T03:02:47.440843Z","shell.execute_reply":"2021-07-19T03:02:51.899174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 1\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm, image_feature in zip(fnm_test[7:12],di_test[7:12]):\n    ## images \n    filename = dir_Flickr_jpg + '/' + jpgfnm\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,1)\n    ax.text(0,0.5,caption,fontsize=20)\n    count += 1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T01:06:17.496749Z","iopub.execute_input":"2021-07-19T01:06:17.497061Z","iopub.status.idle":"2021-07-19T01:06:19.857981Z","shell.execute_reply.started":"2021-07-19T01:06:17.497032Z","shell.execute_reply":"2021-07-19T01:06:19.85706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nindex_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n\n\nnkeep = 5\npred_good, pred_bad, bleus = [], [], [] \ncount = 0 \nfor jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n    count += 1\n    if count % 200 == 0:\n        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n    \n    caption_true = [ index_word[i] for i in tokenized_text ]     \n    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n    ## captions\n    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n    caption = caption.split()\n    caption = caption[1:-1]## remove startreg, and endreg\n    \n    bleu = sentence_bleu([caption_true],caption)\n    bleus.append(bleu)\n    if bleu > 0.7 and len(pred_good) < nkeep:\n        pred_good.append((bleu,jpgfnm,caption_true,caption))\n    elif bleu < 0.3 and len(pred_bad) < nkeep:\n        pred_bad.append((bleu,jpgfnm,caption_true,caption))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:04:10.602592Z","iopub.execute_input":"2021-07-19T03:04:10.603Z","iopub.status.idle":"2021-07-19T03:16:43.596128Z","shell.execute_reply.started":"2021-07-19T03:04:10.602957Z","shell.execute_reply":"2021-07-19T03:16:43.595072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:17:12.096651Z","iopub.execute_input":"2021-07-19T03:17:12.096988Z","iopub.status.idle":"2021-07-19T03:17:12.104961Z","shell.execute_reply.started":"2021-07-19T03:17:12.096957Z","shell.execute_reply":"2021-07-19T03:17:12.104101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(pred_bad):\n    def create_str(caption_true):\n        strue = \"\"\n        for s in caption_true:\n            strue += \" \" + s\n        return(strue)\n    npix = 224\n    target_size = (npix,npix,3)    \n    count = 1\n    fig = plt.figure(figsize=(10,20))\n    npic = len(pred_bad)\n    for pb in pred_bad:\n        bleu,jpgfnm,caption_true,caption = pb\n        ## images \n        filename = dir_Flickr_jpg + '/' + jpgfnm\n        image_load = load_img(filename, target_size=target_size)\n        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        count += 1\n\n        caption_true = create_str(caption_true)\n        caption = create_str(caption)\n        \n        ax = fig.add_subplot(npic,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,1)\n        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n        count += 1\n    plt.show()\n\nprint(\"Bad Caption\")\nplot_images(pred_bad)\nprint(\"Good Caption\")\nplot_images(pred_good)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:17:42.593198Z","iopub.execute_input":"2021-07-19T03:17:42.593561Z","iopub.status.idle":"2021-07-19T03:17:43.922265Z","shell.execute_reply.started":"2021-07-19T03:17:42.59353Z","shell.execute_reply":"2021-07-19T03:17:43.921125Z"},"trusted":true},"execution_count":null,"outputs":[]}]}